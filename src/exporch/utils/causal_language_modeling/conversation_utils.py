from __future__ import annotations

import logging
from typing import Optional

import torch

import transformers
from transformers import (
    AutoModelForCausalLM,
    PreTrainedTokenizer,
    StoppingCriteria,
    BitsAndBytesConfig, AutoTokenizer
)

from exporch.utils.device_utils.device_utils import get_available_device
from exporch.utils.print_utils.print_utils import Verbose


def get_conversation_example_1(
) -> list[str]:
    """
    Retrieves an example conversation for testing the abilities to interact of the model and its knowledge.

    Returns:
        list[str]:
            Example conversation.
    """
    example = ["Hi!",
               "Is Rome the capital of Italy?",
               "Is Paris the capital of France?",
               "Is Barcelona the capital of Spain?",
               "Is Madrid the capital of Russia?",
              ]

    return example


def get_conversation_example_2(
) -> list[str]:
    """
    Retrieves an example conversation for testing the abilities to interact of the model and its memory and reasoning.

    Returns:
        list[str]:
            Example conversation.
    """
    example = ["Hi!",
               "My name is Enrico and I am 24 years old",
               "Football is my favourite sport",
               "Can you tell me my age one year from now?",
               "What is my favourite sport?",
              ]

    return example


class StoppingCriteriaCallback(StoppingCriteria):
    """
    Callback to stop the generation of a language model when calling its "generate" method.

    Args:
        tokenizer (transformers.PreTrainedTokenizer):
            The tokenizer object used for encoding and decoding.
        end_tokens (list[str]):
            A list of end tokens indicating when to stop generation.
        *args:
            Additional arguments to pass to the superclass constructor.
        **kwargs:
            Additional keyword arguments to pass to the superclass constructor.

    Attributes:
        tokenizer (transformers.PreTrainedTokenizer):
            The tokenizer object used for encoding and decoding.
        end_tokens ():
            A list of end tokens indicating when to stop generation.
    """

    def __init__(
            self,
            tokenizer: PreTrainedTokenizer,
            end_tokens: list[str],
            *args,
            **kwargs
    ) -> None:

        super().__init__(*args, **kwargs)
        self.tokenizer = tokenizer
        self.end_tokens = end_tokens

    def __call__(
            self,
            token_ids: torch.LongTensor,
            scores: torch.FloatTensor,
            **kwargs
    ) -> bool:
        """
        Determines whether to stop generation based on the presence of end tokens.

        Args:
            token_ids (torch.Tensor):
                The token IDs generated by the model.
            scores (torch.Tensor):
                The scores associated with the generated input IDs.
            **kwargs:
                Additional keyword arguments.

        Returns:
            bool:
                True if generation should stop, False otherwise.
        """

        decoded_tokens = self.tokenizer.decode(token_ids.squeeze())

        for end_token in self.end_tokens:
            if decoded_tokens.endswith(end_token):
                #print(f"[ DEBUG ] Generation stopped on {end_token} [ DEBUG ]")
                return True

        return False


def start_conversation_loop(
        model: AutoModelForCausalLM,
        tokenizer: PreTrainedTokenizer,
        stop_tokens: list[str] = ("[INST]", "</s>"),
        remove_stop_tokens: list[bool] = (True, False),
        max_len: int = 5,
        user_inputs: Optional[list[str]] = None,
        print_conversation: bool = True,
        make_model_trainable: bool = True
) -> list[dict[str, str]]:
    """
    Allows to have a conversation with the trained model.

    Args:
        model (transformers.AutoModelForCausalLM):
            The trained model.
        tokenizer (transformers.PreTrainedTokenizer):
            Tokenizer object to encode the inputs.
        stop_tokens (list[str]):
            Tokens that, when given as output, stop the model generation of the
            model.
        remove_stop_tokens (list[bool]):
            Flags to remove the stop tokens from the chatbot response.
        max_len (int):
            Maximum length of the conversation. Defaults to 5. If max_len and
            user_inputs are both given, max_len is ignored.
        user_inputs (Optional[list[str]]):
            List of user inputs. If provided, max_len is ignored. Defaults to
            None.
        print_conversation (bool):
            Whether to print the conversation. Defaults to True.
        make_model_trainable (bool):
            Whether to set the model back to train mode after the conversation
            loop. Defaults to True.

    Returns:
        dialogue (str):
            Complete dialogue including user inputs and chatbot responses.
    """

    if len(stop_tokens) != len(remove_stop_tokens):
        raise Exception(
            "The flags about the deletion of the stop tokens from the chatbot response has to be set for only and all "
            "the stop tokens!")

    if user_inputs is not None:
        max_len = len(user_inputs)

    model.eval()

    dialogue = []

    custom_stopping_criteria = StoppingCriteriaCallback(
        tokenizer,
        stop_tokens
    )

    for i in range(max_len):
        # Reading user message
        user_message = ""

        if user_inputs is None:
            user_message = input("User: ")
        else:
            user_message = user_inputs[i]
            if print_conversation:
                print(f"User: {user_inputs[i]}")
        print()

        dialogue.append({
            "role": "user",
            "content": user_message.strip()
        })

        # Encoding input and move it where the model is
        encoded_dialogue = tokenizer.apply_chat_template(
            dialogue,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )

        # Generating model response
        encoded_dialogue = encoded_dialogue.to(model.device)
        output_ids = model.generate(encoded_dialogue,
                                    max_new_tokens=64,
                                    do_sample=True,
                                    top_p=0.9,
                                    top_k=0,
                                    pad_token_id=tokenizer.pad_token_id,
                                    stopping_criteria=[custom_stopping_criteria]
                                    )

        chatbot_response = tokenizer.decode(
            output_ids[0, encoded_dialogue.size(1):],
            skip_special_tokens=False
        )

        stop_token = [el for el in stop_tokens if chatbot_response.endswith(el)]
        if len(stop_token) > 0 and remove_stop_tokens[stop_tokens.index(stop_token[0])]:
            chatbot_response = chatbot_response.rstrip(
                stop_token[0]
            )

        # Appending chatbot response to dialogue history
        dialogue.append({
            "role": "assistant",
            "content": chatbot_response.strip()
        })

        # Printing chatbot response
        if print_conversation:
            print(f"Chatbot: {chatbot_response}")
            print()

    if make_model_trainable:
        model.train()

    return dialogue


def load_model_for_causal_lm(
        config,
) -> transformers.AutoModelForCausalLM:
    """
    Loads the model to be used in the causal language modeling.

    Args:
        config (Config):
            The configuration parameters to use in the loading.

    Returns:
        transformers.AutoModelForCausalLM:
            The model for causal language modeling.
    """

    logger = logging.getLogger(__name__)
    logger.info("Running load_model_for_causal_lm in conversation_utils.py")

    bnb_config = None
    if config.contains("quantization") and config.get("quantization") == "4bit":
        # Defining the quantization configuration (4 bits)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        if config.get_verbose() > Verbose.SILENT:
            print("Loading the model using 4bit quantization\n")
    elif config.contains("quantization") and config.get("quantization") == "8bit":
        # Defining the quantization configuration (8 bits)
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
        )
        if config.get_verbose() > Verbose.SILENT:
            print("Loading the model using 8bit quantization\n")

    torch_dtype = torch.float32
    if config.contains("dtype") and config.get("dtype") == "float16":
        torch_dtype = torch.bfloat16
        config.get_verbose().print("Loading the model using float16 dtype\n", Verbose.INFO)

    model = AutoModelForCausalLM.from_pretrained(
        config.get("model_id"),
        torch_dtype=torch_dtype,
        quantization_config=bnb_config
    )

    if bnb_config is None:
        logger.info(f"{config.contains('device')}")
        logger.info(f"{config.get("device")}")
        logger.info(f"{get_available_device(config.get("device"))}")
        model = model.to(get_available_device(config.get("device") if config.contains("device") else "cpu"))
        logger.info(f"Model loaded on {model.device}")

    config.get_verbose().print("Model loaded.", Verbose.INFO)
    config.get_verbose().print(model, Verbose.INFO)

    return model


def load_tokenizer_for_causal_lm(
        config
) -> [transformers.AutoTokenizer | transformers.PreTrainedTokenizer]:
    """
    Loads the tokenizer to be used in the causal language modeling task.

    Args:
        config (Config):
            The configuration parameters to use in the loading.

    Returns:
        (transformers.AutoTokenizer | transformers.PreTrainedTokenizer):
            The tokenizer for causal language modeling.
    """

    tokenizer = AutoTokenizer.from_pretrained(
        config.get("tokenizer_id")
    )

    if "bert" in config.get("tokenizer_id"):
        tokenizer.bos_token = "[CLS]"
        tokenizer.eos_token = "[SEP]"

    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    config.get_verbose().print("Tokenizer loaded", Verbose.INFO)

    return tokenizer


def load_tokenizer_for_chatbot(
        config
) -> [transformers.AutoTokenizer | transformers.PreTrainedTokenizer]:
    """
    Loads the tokenizer to be used in the causal language modeling task.

    Args:
        config (Config):
            The configuration parameters to use in the loading.

    Returns:
        (transformers.AutoTokenizer | transformers.PreTrainedTokenizer):
            The tokenizer for causal language modeling.
    """

    tokenizer = AutoTokenizer.from_pretrained(
        config.get("tokenizer_id")
    )

    if "bert" in config.get("tokenizer_id"):
        tokenizer.bos_token = "[CLS]"
        tokenizer.eos_token = "[SEP]"

    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    tokenizer.chat_template = AutoTokenizer.from_pretrained(
        config.get("tokenizer_id_for_chat_template")
    ).chat_template

    config.get_verbose().print("Tokenizer loaded", Verbose.INFO)

    return tokenizer
